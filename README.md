# PDF Retrieval-Augmented Generation (RAG) Q&A

This Streamlit app enables question-answering over PDF documents using Retrieval-Augmented Generation (RAG) with HuggingFace LLMs and embeddings.

## Features

- Upload a PDF and preview it in the sidebar
- Automatic text extraction and chunking
- Embedding of document chunks via HuggingFace API
- Semantic search using FAISS for relevant context
- Chat interface for asking questions about your PDF
- Answers generated by a HuggingFace LLM, grounded in document context

## Setup

1. **Install dependencies:**
   ```sh
   pip install streamlit langchain langchain_community faiss-cpu huggingface_hub streamlit_pdf_viewer requests numpy
   ```

2. **Get a HuggingFace token:**
   - Sign up at [HuggingFace](https://huggingface.co/)
   - Get your token from [settings/tokens](https://huggingface.co/settings/tokens)
   - Replace `<Your Token>` in `main.py` with your token

## Usage

1. Run the app:
   ```sh
   streamlit run main.py
   ```
2. Upload a PDF in the sidebar
3. Ask questions about the document in the main area

## Notes

- The app uses HuggingFace Inference API for both embeddings and LLM responses.
- All processing is done locally except for API calls.
- Make sure your HuggingFace token has sufficient permissions.